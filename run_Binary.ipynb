{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85dcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from skfeature.function.similarity_based import fisher_score\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# pd.set_option('max_columns', None)\n",
    "# pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3505851",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4497f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39644, 61)\n",
      "(38463, 61)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.head()\n",
    "#Check Nullity\n",
    "df.isnull().sum() #It appears that the data has no null values\n",
    "#For ' n_tokens_content', if it is 0, it means that the artitle is empty, therefore, let's remove those rows\n",
    "print(df.shape)\n",
    "df = df[df[' n_tokens_content'] != 0]\n",
    "print(df.shape)\n",
    "#Drop columns that are obbiously not going to be useful\n",
    "df.drop(['url', ' timedelta', ' n_non_stop_words', ' n_non_stop_unique_tokens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07ff48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "      <td>38463.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.382419</td>\n",
       "      <td>563.295375</td>\n",
       "      <td>0.565049</td>\n",
       "      <td>11.217872</td>\n",
       "      <td>3.394769</td>\n",
       "      <td>4.563061</td>\n",
       "      <td>1.263786</td>\n",
       "      <td>4.687892</td>\n",
       "      <td>7.215012</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098376</td>\n",
       "      <td>0.779963</td>\n",
       "      <td>-0.267493</td>\n",
       "      <td>-0.537970</td>\n",
       "      <td>-0.110801</td>\n",
       "      <td>0.280573</td>\n",
       "      <td>0.070997</td>\n",
       "      <td>0.342431</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>3355.360398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.113800</td>\n",
       "      <td>468.299538</td>\n",
       "      <td>3.573022</td>\n",
       "      <td>11.340580</td>\n",
       "      <td>3.869773</td>\n",
       "      <td>8.295365</td>\n",
       "      <td>4.164896</td>\n",
       "      <td>0.283231</td>\n",
       "      <td>1.916459</td>\n",
       "      <td>0.226021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070382</td>\n",
       "      <td>0.212509</td>\n",
       "      <td>0.121174</td>\n",
       "      <td>0.279703</td>\n",
       "      <td>0.094919</td>\n",
       "      <td>0.323561</td>\n",
       "      <td>0.264338</td>\n",
       "      <td>0.188606</td>\n",
       "      <td>0.225636</td>\n",
       "      <td>11585.968776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.496250</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.331532</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>945.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>0.542986</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.674121</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.257738</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.861901</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.193415</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_tokens_title   n_tokens_content   n_unique_tokens     num_hrefs  \\\n",
       "count     38463.000000       38463.000000      38463.000000  38463.000000   \n",
       "mean         10.382419         563.295375          0.565049     11.217872   \n",
       "std           2.113800         468.299538          3.573022     11.340580   \n",
       "min           2.000000          18.000000          0.114964      0.000000   \n",
       "25%           9.000000         259.000000          0.477419      5.000000   \n",
       "50%          10.000000         423.000000          0.542986      8.000000   \n",
       "75%          12.000000         729.000000          0.611111     14.000000   \n",
       "max          23.000000        8474.000000        701.000000    304.000000   \n",
       "\n",
       "        num_self_hrefs      num_imgs    num_videos   average_token_length  \\\n",
       "count     38463.000000  38463.000000  38463.000000           38463.000000   \n",
       "mean          3.394769      4.563061      1.263786               4.687892   \n",
       "std           3.869773      8.295365      4.164896               0.283231   \n",
       "min           0.000000      0.000000      0.000000               3.600000   \n",
       "25%           1.000000      1.000000      0.000000               4.496250   \n",
       "50%           3.000000      1.000000      0.000000               4.674121   \n",
       "75%           4.000000      4.000000      1.000000               4.861901   \n",
       "max         116.000000    128.000000     91.000000               8.041534   \n",
       "\n",
       "        num_keywords   data_channel_is_lifestyle  ...   min_positive_polarity  \\\n",
       "count   38463.000000                38463.000000  ...            38463.000000   \n",
       "mean        7.215012                    0.054000  ...                0.098376   \n",
       "std         1.916459                    0.226021  ...                0.070382   \n",
       "min         1.000000                    0.000000  ...                0.000000   \n",
       "25%         6.000000                    0.000000  ...                0.050000   \n",
       "50%         7.000000                    0.000000  ...                0.100000   \n",
       "75%         9.000000                    0.000000  ...                0.100000   \n",
       "max        10.000000                    1.000000  ...                1.000000   \n",
       "\n",
       "        max_positive_polarity   avg_negative_polarity   min_negative_polarity  \\\n",
       "count            38463.000000            38463.000000            38463.000000   \n",
       "mean                 0.779963               -0.267493               -0.537970   \n",
       "std                  0.212509                0.121174                0.279703   \n",
       "min                  0.000000               -1.000000               -1.000000   \n",
       "25%                  0.600000               -0.331532               -0.714286   \n",
       "50%                  0.800000               -0.257738               -0.500000   \n",
       "75%                  1.000000               -0.193415               -0.312500   \n",
       "max                  1.000000                0.000000                0.000000   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "count            38463.000000         38463.000000               38463.000000   \n",
       "mean                -0.110801             0.280573                   0.070997   \n",
       "std                  0.094919             0.323561                   0.264338   \n",
       "min                 -1.000000             0.000000                  -1.000000   \n",
       "25%                 -0.125000             0.000000                   0.000000   \n",
       "50%                 -0.100000             0.125000                   0.000000   \n",
       "75%                 -0.050000             0.500000                   0.136364   \n",
       "max                  0.000000             1.000000                   1.000000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity         shares  \n",
       "count             38463.000000                   38463.000000   38463.000000  \n",
       "mean                  0.342431                       0.154930    3355.360398  \n",
       "std                   0.188606                       0.225636   11585.968776  \n",
       "min                   0.000000                       0.000000       1.000000  \n",
       "25%                   0.166667                       0.000000     945.000000  \n",
       "50%                   0.500000                       0.000000    1400.000000  \n",
       "75%                   0.500000                       0.250000    2700.000000  \n",
       "max                   0.500000                       1.000000  843300.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2b7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945.0\n",
      "1400.0\n",
      "2700.0\n"
     ]
    }
   ],
   "source": [
    "#25 Percentile\n",
    "print(df[' shares'].quantile(q=0.25))\n",
    "\n",
    "#50 Percentile\n",
    "print(df[' shares'].quantile(q=0.50))\n",
    "\n",
    "#75 Percentile\n",
    "print(df[' shares'].quantile(q=0.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c062c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400.0\n"
     ]
    }
   ],
   "source": [
    "#Convert into binary classfication problem\n",
    "#Use median to create label: 0:bad 1:good\n",
    "print(df[' shares'].median())\n",
    "df[' popularity'] = df[' shares'].apply(lambda x: 0 if x <1400 else 1) \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1066aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Some of the features are related to each other such as ' weekday_is_monday' and ' weekday_is_tuesday' and ' data_channel_is_lifestyle' and ' data_channel_is_entertainment'. This violates non-multicollinearity of model such as linear regression\n",
    "#Merging \n",
    "dayMerge = df[[' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday',' weekday_is_thursday',' weekday_is_friday',' weekday_is_saturday',' weekday_is_sunday', ]]\n",
    "day_arr = []\n",
    "for r in list(range(dayMerge.shape[0])):\n",
    "    for c in list(range(dayMerge.shape[1])):\n",
    "        if ((c==0) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(0)\n",
    "        elif ((c==1) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(1)\n",
    "        elif ((c==2) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(2)\n",
    "        elif ((c==3) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(3)\n",
    "        elif ((c==4) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(4)\n",
    "        elif ((c==5) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(5) \n",
    "        elif ((c==6) and (dayMerge.iloc[r,c])==1):\n",
    "            day_arr.append(6)\n",
    "            \n",
    "channelMerge=df[[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n",
    "                        ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world' ]]\n",
    "\n",
    "channel_arr = []\n",
    "\n",
    "for r in list(range(channelMerge.shape[0])):\n",
    "    if (((channelMerge.iloc[r,0])==0) and ((channelMerge.iloc[r,1])==0) and ((channelMerge.iloc[r,2])==0) and ((channelMerge.iloc[r,3])==0) and ((channelMerge.iloc[r,4])==0) and ((channelMerge.iloc[r,5])==0)):\n",
    "        channel_arr.append(6)\n",
    "    for c in list(range(channelMerge.shape[1])):\n",
    "        if ((c==0) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(0)\n",
    "        elif ((c==1) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(1)\n",
    "        elif ((c==2) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(2)\n",
    "        elif ((c==3) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(3)\n",
    "        elif ((c==4) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(4)\n",
    "        elif ((c==5) and (channelMerge.iloc[r,c])==1):\n",
    "            channel_arr.append(5)\n",
    "            \n",
    "df.insert(loc=11, column='weekdays', value=day_arr)\n",
    "df.insert(loc=12, column='data_channel', value=channel_arr)\n",
    "\n",
    "df.drop(labels=[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n",
    "                  ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world', \n",
    "                  ' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday', \n",
    "                  ' weekday_is_thursday', ' weekday_is_friday',' weekday_is_saturday' ,' weekday_is_sunday', ' shares'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92c0255",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'popularity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Check Class Balance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpopularity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      3\u001b[0m class_counts\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopularity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo of articles\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_counts)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:7712\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7707\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   7709\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[1;32m   7710\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[1;32m   7711\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[0;32m-> 7712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7715\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7718\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7720\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   7721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/groupby/grouper.py:882\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    880\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'popularity'"
     ]
    }
   ],
   "source": [
    "#Check Class Balance\n",
    "class_counts = df.groupby('popularity').size().reset_index()\n",
    "class_counts.columns = ['Popularity','No of articles']\n",
    "print(class_counts)\n",
    "\n",
    "weekdays_data = df.groupby('weekdays').size().reset_index()\n",
    "weekdays_data.columns = ['weekdays','count']\n",
    "print(weekdays_data)\n",
    "\n",
    "channel_data = df.groupby('data_channel').size().reset_index()\n",
    "channel_data.columns = ['data_channel','count']\n",
    "print(channel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c881c3",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446a09d",
   "metadata": {},
   "source": [
    "## L1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589991f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting Lasso(L1) feature selection\n",
    "# shares data is not needed for classification\n",
    "X = df.drop(labels=['popularity'], axis = 1, inplace=False)\n",
    "y = df['popularity']\n",
    "X= pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the train and test sample\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Perform GridSearchCV to tune best-fit LR model\n",
    "param = {'C': [10**-2,10**-1,10**0,10**1,10**2]}\n",
    "\n",
    "lr_model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Redundant Feature Count: \", sum(lr_model.coef_[0]==0))\n",
    "print(\"Redundant Feature Names: \", list(pd.Series(X_train.columns)[list(lr_model.coef_[0]==0)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8555ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=[' global_rate_negative_words', ' avg_positive_polarity', ' global_sentiment_polarity', ' avg_negative_polarity', ' max_negative_polarity'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101c68d",
   "metadata": {},
   "source": [
    "## Fisher Score Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X = df.drop(labels=['popularity'], axis = 1, inplace=False)\n",
    "# y = df['popularity']\n",
    "# X = pd.get_dummies(X)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = False)\n",
    "# X_std = scaler.fit_transform(X_train)\n",
    "# from skfeature.function.similarity_based import fisher_score\n",
    "# ranks = fisher_score.fisher_score(X_std, y_train)\n",
    "# print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# feat_importance = pd.Series(ranks, X.columns[0: len(X.columns)])\n",
    "# feat_importance.plot(kind='barh', color = 'teal')\n",
    "# fig = plt.figure() \n",
    "# fig.set_size_inches(50, 50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_lst = []\n",
    "# score_map = []\n",
    "# for idx, val in enumerate(ranks):\n",
    "#     if val < 5:\n",
    "#         idx_lst.append(idx)\n",
    "#         score_map.append((idx, val))\n",
    "# print(score_map)\n",
    "# print(\"Redundant Feature Names: \", list(pd.Series(X_train.columns)[idx_lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(labels=list(pd.Series(X_train.columns)[idx_lst]), axis = 1, inplace=True)\n",
    "# df.drop(labels=[' global_rate_negative_words', ' avg_positive_polarity', ' global_sentiment_polarity', ' avg_negative_polarity', ' max_negative_polarity'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0cc2b3",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_std = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(X_std, columns = df.columns[0: len(df.columns)])\n",
    "X = df.drop(labels=['popularity'], axis = 1, inplace=False)\n",
    "y = df['popularity']\n",
    "labelEn = LabelEncoder()\n",
    "y = labelEn.fit_transform(y)\n",
    "# class_names = labelEn.classes_\n",
    "\n",
    "X= pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17d3d6",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC(gamma='auto')\n",
    "# svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcddb0",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3d715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=7000)\n",
    "# logistic_regression.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0daf27",
   "metadata": {},
   "source": [
    "## Tree-based Method: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9389e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classfication = RandomForestClassifier(n_estimators=1000, n_jobs=-1, max_depth=50,random_state=666)\n",
    "rf_classfication.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554e174",
   "metadata": {},
   "source": [
    "## Tree-based Method: XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9970e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "             min_child_weight=6, monotone_constraints='()',\n",
    "       n_estimators=400, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
    "            reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "xgboost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34effa",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd582b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as T\n",
    "# class Net(T.nn.Module):\n",
    "#   def __init__(self):\n",
    "#     super(Net, self).__init__()\n",
    "#     self.hid1 = T.nn.Linear(8, 10)  # 8-(10-10)-1\n",
    "#     self.hid2 = T.nn.Linear(10, 10)\n",
    "#     self.oupt = T.nn.Linear(10, 1)\n",
    "\n",
    "#     T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "#     T.nn.init.zeros_(self.hid1.bias)\n",
    "#     T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "#     T.nn.init.zeros_(self.hid2.bias)\n",
    "#     T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "#     T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     z = T.relu(self.hid1(x))\n",
    "#     z = T.relu(self.hid2(z))\n",
    "#     z = self.oupt(z)  # no activation\n",
    "#     return z\n",
    "\n",
    "# bat_size = 10\n",
    "# net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bb3bd",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluateing Logistic Regression\n",
    "# y_pred = logistic_regression.predict(X_test)\n",
    "# print(\"F1 Score: \",f1_score(y_test, y_pred, average='macro'))\n",
    "#print(\"Accuracy: \",accuracy_score(y_test, y_pred))\n",
    "#F1 Score:  0.31860939565338836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb65e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating RF\n",
    "y_pred = rf_classfication.predict(X_test)\n",
    "print(\"F1 Score: \",f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Accuracy: \",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating SVM\n",
    "# y_pred = svm.predict(X_test)\n",
    "# print(\"F1 Score: \",f1_score(y_test, y_pred, average='macro'))\n",
    "#print(\"Accuracy: \",accuracy_score(y_test, y_pred))\n",
    "#F1 Score:  0.10067998398348055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating XG Boost\n",
    "y_pred = xgboost.predict(X_test)\n",
    "print(\"F1 Score: \",f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Accuracy: \",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968243c",
   "metadata": {},
   "source": [
    "# Conclusions and Findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
